{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络：一步一步\n",
    "\n",
    "欢迎来到课程 4 的第一个作业！ 在本作业中，您将在 numpy 中实现卷积 (CONV) 和池化 (POOL) 层，包括前向传播和（可选）反向传播。\n",
    "\n",
    "**符号**:\n",
    "- 上标 $[l]$ 表示  $l^{th}$ 层的一个对象. \n",
    "    - 示例：$a^{[4]}$ 是 $4^{th}$ 层激活。 $W^{[5]}$ 和 $b^{[5]}$ 是 $5^{th}$ 层参数。\n",
    "\n",
    "\n",
    "- 上标 $(i)$ 表示来自 $i^{th}$ 示例的对象。\n",
    "    - 示例：$x^{(i)}$ 是 $i^{th}$ 训练示例输入。\n",
    "    \n",
    "    \n",
    "- 下标 $i$ 表示向量的 $i^{th}$ 项。\n",
    "    - 示例：$a^{[l]}_i$ 表示 $l$ 层中激活的 $i^{th}$ 条目，假设这是一个全连接（FC）层。\n",
    "    \n",
    "    \n",
    "- $n_H$、$n_W$ 和 $n_C$ 分别表示给定层的高度、宽度和通道数。 如果要引用特定层$l$，也可以写成$n_H^{[l]}$, $n_W^{[l]}$, $n_C^{[l]}$。\n",
    "- $n_{H_{prev}}$, $n_{W_{prev}}$ 和 $n_{C_{prev}}$ 分别表示前一层的高度、宽度和通道数。如果引用特定层 $l$，这也可以表示为 $n_H^{[l-1]}$、$n_W^{[l-1]}$、$n_C^{[l-1]}$。\n",
    "\n",
    "我们假设您已经熟悉“numpy”和/或已经完成了以前的专业课程。 让我们开始吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages\n",
    "\n",
    "让我们首先导入您在此任务期间需要的所有包。\n",
    "- [numpy](www.numpy.org) 是使用 Python 进行科学计算的基础包。\n",
    "- [matplotlib](http://matplotlib.org) 是一个在 Python 中绘制图形的库。\n",
    "- np.random.seed(1) 用于保持所有随机函数调用一致。 它将帮助我们为您的作品评分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 作业大纲\n",
    "\n",
    "您将实现卷积神经网络的构建块！ 您将实现的每个功能都有详细的说明，将引导您完成所需的步骤：\n",
    "\n",
    "- 卷积函数，包括：\n",
    "     - 零填充\n",
    "     - 卷积窗口\n",
    "     - 前向卷积\n",
    "     - 向后卷积（可选）\n",
    "- 池化功能，包括：\n",
    "     - 向前汇集\n",
    "     - 创建面具\n",
    "     - 分配价值\n",
    "     - 向后汇集（可选）\n",
    "    \n",
    "本笔记本将要求您在 `numpy` 中从头开始实现这些功能。 在下一个笔记本中，您将使用这些函数的 TensorFlow 等效项来构建以下模型：\n",
    "\n",
    "<img src=\"images/model.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "**注意** 对于每个前向函数，都有其对应的后向等效函数。 因此，在前向模块的每一步，您都会将一些参数存储在缓存中。 这些参数用于在反向传播期间计算梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - 卷积神经网络\n",
    "\n",
    "尽管编程框架使卷积易于使用，但它们仍然是深度学习中最难理解的概念之一。 卷积层将输入体积转换为不同大小的输出体积，如下所示。\n",
    "\n",
    "<img src=\"images/conv_nn.png\" style=\"width:350px;height:200px;\">\n",
    "\n",
    "在这一部分中，您将构建卷积层的每一步。 您将首先实现两个辅助函数：一个用于零填充，另一个用于计算卷积函数本身。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - 零填充\n",
    "\n",
    "零填充在图像边界周围添加零：\n",
    "\n",
    "<img src=\"images/PAD.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 1** </u><font color='purple'>  : **Zero-Padding**<br> Image (3 channels, RGB) with a padding of 2. </center></caption>\n",
    "\n",
    "填充的主要好处如下：\n",
    "\n",
    "- 它允许您使用 CONV 层，而不必缩小体积的高度和宽度。这对于构建更深的网络很重要，否则高度/宽度会随着您进入更深的层而缩小。一个重要的特殊情况是“相同”卷积，其中高度/宽度在一层之后完全保留。\n",
    "\n",
    "- 它可以帮助我们将更多信息保留在图像的边缘。如果没有填充，下一层的值很少会受到作为图像边缘的像素的影响。\n",
    "\n",
    "**Exercise**：实现以下函数，用零填充一批示例 X 的所有图像。 [使用 np.pad](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html)。请注意，如果要填充形状为 $(5,5,5,5,5)$ 的数组“a”，第 2 维使用 `pad = 1`，第 4 维使用 `pad = 3`并且`pad = 0` 剩下的，你会这样做：\n",
    "```python\n",
    "a = np.pad(a, ((0,0), (1,1), (0,0), (3,3), (0,0)), 'constant', constant_values = (..,..))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分级功能：zero_pad\n",
    "\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "     用零填充数据集 X 的所有图像。填充应用于图像的高度和宽度，\n",
    "     如图 1 所示。\n",
    "    \n",
    "     论点：\n",
    "     X - python numpy 形状数组 (m, n_H, n_W, n_C) 表示一批 m 图像\n",
    "     pad -- 整数，每个图像在垂直和水平维度上的填充量\n",
    "    \n",
    "     返回值：\n",
    "     X_pad -- 形状的填充图像 (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "     \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line)\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=0)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (4, 3, 3, 2)\n",
      "x_pad.shape = (4, 7, 7, 2)\n",
      "x[1,1] = [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] = [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x290a822c9a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print (\"x.shape =\", x.shape)\n",
    "print (\"x_pad.shape =\", x_pad.shape)\n",
    "print (\"x[1,1] =\", x[1,1])\n",
    "print (\"x_pad[1,1] =\", x_pad[1,1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **x.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (4, 3, 3, 2)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **x_pad.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (4, 7, 7, 2)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **x[1,1]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [[ 0.90085595 -0.68372786]\n",
    " [-0.12289023 -0.93576943]\n",
    " [-0.26788808  0.53035547]]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **x_pad[1,1]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [[ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - 单步卷积\n",
    "\n",
    "在这一部分中，实现单步卷积，将过滤器应用于输入的单个位置。 这将用于构建一个卷积单元，它：\n",
    "\n",
    "- 接受输入集\n",
    "- 在输入的每个位置应用过滤器\n",
    "- 输出另一个集（通常大小不同）\n",
    "\n",
    "<img src=\"images/Convolution_schematic.gif\" style=\"width:500px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 2** </u><font color='purple'>  : **Convolution operation**<br> with a filter of 2x2 and a stride of 1 (stride = amount you move the window each time you slide) </center></caption>\n",
    "\n",
    "在计算机视觉应用程序中，左侧矩阵中的每个值对应一个像素值，我们通过将其值与原始矩阵逐元素相乘，然后将它们相加并添加偏差，将 3x3 滤波器与图像进行卷积 . 在练习的第一步中，您将实现单步卷积，对应于仅对其中一个位置应用过滤器以获得单个实值输出。\n",
    "\n",
    "稍后在本笔记本中，您将将此函数应用于输入的多个位置以实现完整的卷积操作。 \n",
    "\n",
    "**Exercise**: Implement conv_single_step(). [Hint](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sum.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分级功能：conv_single_step\n",
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    在前一层的输出激活的单个切片 (a_slice_prev) 上应用由参数 W 定义的过滤器。\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- 形状 (f, f, n_C_prev) 的输入数据切片\n",
    "    W -- 窗口中包含的权重参数 - 形状矩阵 (f, f, n_C_prev)\n",
    "    b -- 窗口中包含的偏置参数 - 形状矩阵 (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- 一个标量值，在输入数据的切片 x 上对滑动窗口 (W, b) 进行卷积的结果\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # a_slice_prev 和 W 之间的元素乘积。暂时不要添加偏差。\n",
    "    s = np.multiply(a_slice_prev,W)\n",
    "    # 对卷积 s 的所有项求和。\n",
    "    Z = np.sum(s)\n",
    "    # 将偏差 b 添加到 Z。将 b 转换为 float()，以便 Z 产生一个标量值。\n",
    "    Z = Z + float(b)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -6.99908945068\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Z**\n",
    "        </td>\n",
    "        <td>\n",
    "            -6.99908945068\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.3 - 卷积神经网络 - 向前传播\n",
    "\n",
    "在前向传递中，您将采用许多过滤器并将它们与输入进行卷积。 每个“卷积”都会为您提供 2D 矩阵输出。 然后，您将堆叠这些输出以获得 3D 体积：\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/conv_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "**练习**：实现下面的函数以在输入激活 A_prev 上卷积过滤器 W。 该函数将输入 A_prev、前一层的激活输出（对于 m 个输入的批次）、用 W 表示的 F 个过滤器/权重和用 b 表示的偏置向量，其中每个过滤器都有自己的（单个）偏置。 最后，您还可以访问包含步幅和填充的超参数字典。\n",
    "\n",
    "**示意**: \n",
    "1. 要在矩阵“a_prev”（形状 (5,5,3)）的左上角选择一个 2x2 切片，您可以：\n",
    "```python\n",
    "a_slice_prev = a_prev[0:2,0:2,:]\n",
    "```\n",
    "当您使用您将定义的 `start/end` 索引在下面定义 `a_slice_prev` 时，这将很有用。\n",
    "2. 要定义 a_slice，您首先需要定义它的角 `vert_start`、`vert_end`、`horiz_start` 和 `horiz_end`。 此图可能有助于您了解如何在下面的代码中使用 h、w、f 和 s 定义每个角。\n",
    "\n",
    "<img src=\"images/vert_horiz_kiank.png\" style=\"width:400px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **图形 3** </u><font color='purple'>  : **使用垂直和水平开始/结束定义切片（使用 2x2 过滤器）** <br> 此图仅显示单个通道。 </center></caption>\n",
    "\n",
    "\n",
    "**提醒**:\n",
    "将卷积的输出形状与输入形状相关联的公式是：\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_C = \\text{卷积所用的过滤器数量}$$\n",
    "\n",
    "在这个练习中，我们不会担心向量化，而是会使用 for 循环来实现所有内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分级功能：conv_forward\n",
    "\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    实现卷积函数的前向传播\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- 上一层的输出激活，numpy 数组形状(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- 权重, numpy 数组形状 (f, f, n_C_prev, n_C)\n",
    "    b -- 偏差, numpy 数组形状 (1, 1, 1, n_C)\n",
    "    超参 -- python 字典 包含 \"步长\" 和 \"填充\"\n",
    "        \n",
    "    返回值:\n",
    "    Z -- 卷积输出, numpy数组的形状(m, n_H, n_W, n_C)\n",
    "    cache -- 返回conv_backward()函数。需要的变量值的缓存\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # 从A_prev 的形状 取出 尺寸(≈1 line)  \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # 从 W's shape 的形状 取出 尺寸(≈1 line)\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # 从 \"hparameters\"  取出 内容(≈2 lines)\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    #使用上面给出的公式计算 CONV 输出体积的尺寸。 提示：使用 int() 来设置。 （≈2 行）\n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "    \n",
    "    # 用零初始化输出 Z。 (≈1 line)\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # 通过填充 A_prev 创建 A_prev_pad\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):                               # 遍历这批训练样本\n",
    "        a_prev_pad = A_prev_pad[i]                     # 选择第 i 个训练示例的填充激活\n",
    "        for h in range(n_H):                           # 在输出卷积的垂直轴上循环\n",
    "            for w in range(n_W):                       # 在输出卷积的横轴上循环\n",
    "                for c in range(n_C):                   # 循环输出卷积的通道（= #filters）\n",
    "                    \n",
    "                    # 找到当前“切片”的角点（≈4 行）\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # 使用角来定义 a_prev_pad 的 (3D) 切片（参见单元格上方的提示）。 (≈1 行)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    \n",
    "                    # 将 (3D) 切片与正确的滤波器 W 和偏置 b 进行卷积，以获得一个输出神经元。 (≈1 行)\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[...,c], b[...,c])\n",
    "                                        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # 确保输出的形状是对的\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # 为了后面的反向传播，现在保存一些值到缓存内。\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean = 0.0489952035289\n",
      "Z[3,2,1] = [-0.61490741 -6.7439236  -2.55153897  1.75698377  3.56208902  0.53036437\n",
      "  5.18531798  8.75898442]\n",
      "cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\", np.mean(Z))\n",
    "print(\"Z[3,2,1] =\", Z[3,2,1])\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Z's mean**\n",
    "        </td>\n",
    "        <td>\n",
    "            0.0489952035289\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Z[3,2,1]**\n",
    "        </td>\n",
    "        <td>\n",
    "            [-0.61490741 -6.7439236  -2.55153897  1.75698377  3.56208902  0.53036437\n",
    "  5.18531798  8.75898442]\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cache_conv[0][1][2][3]**\n",
    "        </td>\n",
    "        <td>\n",
    "            [-0.20075807  0.18656139  0.41005165]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，CONV 层还应该包含一个激活，在这种情况下，我们将添加以下代码行：\n",
    "\n",
    "```python\n",
    "# 卷积窗口以取回一个输出神经元\n",
    "Z[i, h, w, c] = ...\n",
    "# 申请激活\n",
    "A[i, h, w, c] = activation(Z[i, h, w, c])\n",
    "```\n",
    "\n",
    "你不需要在这里做。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - 池化层\n",
    "\n",
    "池化（POOL）层减少了输入的高度和宽度。 它有助于减少计算量，并有助于使特征检测器对其在输入中的位置更加不变。 两种类型的池化层是： \n",
    "\n",
    "- 最大池化层：在输入上滑动一个 ($f, f$) 窗口并将窗口的最大值存储在输出中。\n",
    "\n",
    "- 平均池化层：在输入上滑动一个 ($f, f$) 窗口并将窗口的平均值存储在输出中。\n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/max_pool1.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "\n",
    "<td>\n",
    "<img src=\"images/a_pool.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "</table>\n",
    "\n",
    "这些池化层没有用于反向传播训练的参数。 但是，它们具有超参数，例如窗口大小 $f$。 这指定了您将计算最大值或平均值的 fxf 窗口的高度和宽度。\n",
    "\n",
    "### 4.1 - 前向池化\n",
    "现在，您将在同一个函数中实现 MAX-POOL 和 AVG-POOL。\n",
    "\n",
    "**训练**: 实现池化层的前向传递。 按照下面评论中的提示进行操作。<br>\n",
    "**提醒**:\n",
    "由于没有填充，将池的输出形状绑定到输入形状的公式是：\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "$$ n_C = n_{C_{prev}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分级功能：pool_forward\n",
    "\n",
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    实现池化层的前向传递\n",
    "    \n",
    "     论据：\n",
    "     A_prev -- 输入数据，numpy 形状数组 (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "     hparameters - 包含“f”和“stride”的python字典\n",
    "     mode -- 您想使用的池化模式，定义为字符串（“max”或“average”）\n",
    "    \n",
    "     返回:\n",
    "     A -- 池层的输出，一个形状为 (m, n_H, n_W, n_C) 的 numpy 数组\n",
    "     cache -- 用于池化层后向传递的缓存，包含输入和 h 参数\n",
    "    \"\"\"\n",
    "    \n",
    "    # 从输入形状中检索尺寸\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # 从“hparameters”中检索超参数\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # 定义输出的维度\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # 初始化输出矩阵 A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(m):                         # 循环训练集\n",
    "        for h in range(n_H):                     # 在输出卷积的垂直轴方向循环\n",
    "            for w in range(n_W):                 # 在输出卷积的横轴方向循环\n",
    "                for c in range (n_C):            # 循环输出卷积的通道\n",
    "                    \n",
    "                    # 找到当前切片的4个角(≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # 使用角点定义 A_prev 的第 i 个训练示例、通道 c 上的当前切片。 (≈1 行)\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # 计算切片上的池化操作。 使用 if 语句来区分模式。 使用 np.max/np.mean。\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # 将输入和 h 参数存储在 pool_backward() 的“缓存”中\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # 确保您的输出形状正确\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A = [[[[1.74481176 0.86540763 1.13376944]]]\n",
      "\n",
      "\n",
      " [[[1.13162939 1.51981682 2.18557541]]]]\n",
      "\n",
      "mode = average\n",
      "A = [[[[ 0.02105773 -0.20328806 -0.40389855]]]\n",
      "\n",
      "\n",
      " [[[-0.22154621  0.51716526  0.48155844]]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 4, 4, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A =\", A)\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A =\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            A  =\n",
    "        </td>\n",
    "        <td>\n",
    "             [[[[ 1.74481176  0.86540763  1.13376944]]]<br>\n",
    "            [[[ 1.13162939  1.51981682  2.18557541]]]]\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            A  =\n",
    "        </td>\n",
    "        <td>\n",
    "             [[[[ 0.02105773 -0.20328806 -0.40389855]]]<br>\n",
    "            [[[-0.22154621  0.51716526  0.48155844]]]]\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜！ 您现在已经实现了卷积网络所有层的前向传递。\n",
    "\n",
    "这个笔记本的剩余部分是可选的，不会被评分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - 卷积神经网络中的反向传播（可选/未分级）\n",
    "\n",
    "在现代深度学习框架中，你只需要实现前向传递，而框架负责后向传递，因此大多数深度学习工程师不需要关心后向传递的细节。 卷积网络的反向传播很复杂。 但是，如果您愿意，您可以通过笔记本的这个可选部分来了解卷积网络中的反向传播是什么样的。\n",
    "\n",
    "在之前的课程中，您实现了一个简单的（完全连接的）神经网络，您使用反向传播来计算与更新参数成本相关的导数。 同样，在卷积神经网络中，您可以计算相对于成本的导数以更新参数。 反向传播方程并非微不足道，我们在讲座中没有推导出它们，但我们在下面简要介绍了它们。\n",
    "\n",
    "### 5.1 - 卷积层反向传播 \n",
    "\n",
    "让我们从实现 CONV 层的反向传播开始。\n",
    "\n",
    "#### 5.1.1 - 计算 dA:\n",
    "这是针对某个过滤器 $W_c$ 和给定训练示例的成本计算 $dA$ 的公式：\n",
    "\n",
    "$$ dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "\n",
    "其中$W_c$是一个过滤器，$dZ_{hw}$是一个标量，对应于conv层Z在第h行第w列的输出的代价梯度（对应于在第 第 i 步向左，第 j 步向下）。 请注意，每次更新 dA 时，我们将相同的过滤器 $W_c$ 乘以不同的 dZ。 我们这样做主要是因为在计算前向传播时，每个过滤器都由不同的 a_slice 点和求和。 因此，在计算 dA 的反向传播时，我们只是将所有 a_slices 的梯度相加。\n",
    "\n",
    "在代码中，在适当的 for 循环中，此公式转换为：\n",
    "```python\n",
    "da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### 5.1.2 - 计算 dW:\n",
    "这是计算关于损失的 $dW_c$（$dW_c$ 是一个滤波器的导数）的公式：\n",
    "\n",
    "$$ dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n",
    "\n",
    "其中 $a_{slice}$ 对应于用于生成激活 $Z_{ij}$ 的切片。 因此，这最终为我们提供了 $W$ 相对于该切片的梯度。 由于它是相同的 $W$，我们只需将所有这些梯度相加即可得到 $dW$。\n",
    "\n",
    "在代码中，在适当的 for 循环中，此公式转换为：\n",
    "```python\n",
    "dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### 5.1.3 - 计算 db:\n",
    "\n",
    "这是针对某个过滤器 $W_c$ 的成本计算 $db$ 的公式：\n",
    "\n",
    "$$ db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$\n",
    "\n",
    "正如您之前在基本神经网络中看到的，db 是通过对 $dZ$ 求和来计算的。 在这种情况下，您只是对 conv 输出 (Z) 相对于成本的所有梯度求和。\n",
    "\n",
    "在代码中，在适当的 for 循环中，此公式转换为：\n",
    "```python\n",
    "db[:,:,:,c] += dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "**练习**：实现下面的`conv_backward`函数。 您应该总结所有训练示例、过滤器、高度和宽度。 然后，您应该使用上面的公式 1、2 和 3 计算导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    实现卷积函数的反向传播\n",
    "    \n",
    "     论据：\n",
    "     dZ - 成本相对于卷积层输出的梯度 (Z)，numpy 形状数组 (m, n_H, n_W, n_C)\n",
    "     cache -- conv_backward() 所需值的缓存，conv_forward() 的输出\n",
    "    \n",
    "     返回：\n",
    "     dA_prev——成本相对于卷积层输入的梯度（A_prev），\n",
    "                numpy 形状数组（m，n_H_prev，n_W_prev，n_C_prev）\n",
    "     dW——成本相对于卷积层权重的梯度（W）\n",
    "           numpy 形状数组 (f, f, n_C_prev, n_C)\n",
    "     db -- 成本相对于 conv 层偏差的梯度 (b)\n",
    "           numpy 形状数组 (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 0.634770447265\n",
      "dW_mean = 1.55726574285\n",
      "db_mean = 7.83923256462\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output: **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **dA_mean**\n",
    "        </td>\n",
    "        <td>\n",
    "            1.45243777754\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **dW_mean**\n",
    "        </td>\n",
    "        <td>\n",
    "            1.72699145831\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **db_mean**\n",
    "        </td>\n",
    "        <td>\n",
    "            7.83923256462\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Pooling layer - backward pass\n",
    "\n",
    "Next, let's implement the backward pass for the pooling layer, starting with the MAX-POOL layer. Even though a pooling layer has no parameters for backprop to update, you still need to backpropagation the gradient through the pooling layer in order to compute gradients for layers that came before the pooling layer. \n",
    "\n",
    "### 5.2.1 Max pooling - backward pass  \n",
    "\n",
    "Before jumping into the backpropagation of the pooling layer, you are going to build a helper function called `create_mask_from_window()` which does the following: \n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "1 && 3 \\\\\n",
    "4 && 2\n",
    "\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n",
    "0 && 0 \\\\\n",
    "1 && 0\n",
    "\\end{bmatrix}\\tag{4}$$\n",
    "\n",
    "As you can see, this function creates a \"mask\" matrix which keeps track of where the maximum of the matrix is. True (1) indicates the position of the maximum in X, the other entries are False (0). You'll see later that the backward pass for average pooling will be similar to this but using a different mask.  \n",
    "\n",
    "**Exercise**: Implement `create_mask_from_window()`. This function will be helpful for pooling backward. \n",
    "Hints:\n",
    "- [np.max()]() may be helpful. It computes the maximum of an array.\n",
    "- If you have a matrix X and a scalar x: `A = (X == x)` will return a matrix A of the same size as X such that:\n",
    "```\n",
    "A[i,j] = True if X[i,j] = x\n",
    "A[i,j] = False if X[i,j] != x\n",
    "```\n",
    "- Here, you don't need to consider cases where there are several maxima in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈1 line)\n",
    "    mask = x == np.max(x)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2,3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Expected Output:** \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "\n",
    "**x =**\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "[[ 1.62434536 -0.61175641 -0.52817175] <br>\n",
    " [-1.07296862  0.86540763 -2.3015387 ]]\n",
    "\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "<td>\n",
    "**mask =**\n",
    "</td>\n",
    "<td>\n",
    "[[ True False False] <br>\n",
    " [False False False]]\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we keep track of the position of the max? It's because this is the input value that ultimately influenced the output, and therefore the cost. Backprop is computing gradients with respect to the cost, so anything that influences the ultimate cost should have a non-zero gradient. So, backprop will \"propagate\" the gradient back to this particular input value that had influenced the cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 - Average pooling - backward pass \n",
    "\n",
    "In max pooling, for each input window, all the \"influence\" on the output came from a single input value--the max. In average pooling, every element of the input window has equal influence on the output. So to implement backprop, you will now implement a helper function that reflects this.\n",
    "\n",
    "For example if we did average pooling in the forward pass using a 2x2 filter, then the mask you'll use for the backward pass will look like: \n",
    "$$ dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix}\n",
    "1/4 && 1/4 \\\\\n",
    "1/4 && 1/4\n",
    "\\end{bmatrix}\\tag{5}$$\n",
    "\n",
    "This implies that each position in the $dZ$ matrix contributes equally to output because in the forward pass, we took an average. \n",
    "\n",
    "**Exercise**: Implement the function below to equally distribute a value dz through a matrix of dimension shape. [Hint](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ones.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = dz / (n_H * n_W)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    a = np.ones(shape) * average\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[ 0.5  0.5]\n",
      " [ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2,2))\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "distributed_value =\n",
    "</td>\n",
    "<td>\n",
    "[[ 0.5  0.5]\n",
    "<br\\> \n",
    "[ 0.5  0.5]]\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Putting it together: Pooling backward \n",
    "\n",
    "You now have everything you need to compute backward propagation on a pooling layer.\n",
    "\n",
    "**Exercise**: Implement the `pool_backward` function in both modes (`\"max\"` and `\"average\"`). You will once again use 4 for-loops (iterating over training examples, height, width, and channels). You should use an `if/elif` statement to see if the mode is equal to `'max'` or `'average'`. If it is equal to 'average' you should use the `distribute_value()` function you implemented above to create a matrix of the same shape as `a_slice`. Otherwise, the mode is equal to '`max`', and you will create a mask with `create_mask_from_window()` and multiply it by the corresponding value of dZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        da = dA[i, h, w, c]\n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f, f)\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
    "                        \n",
    "    ### END CODE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.145713902729\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.145713902729\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "mode = max:\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "\n",
    "**mean of dA =**\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "0.145713902729\n",
    "\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "<td>\n",
    "**dA_prev[1,1] =** \n",
    "</td>\n",
    "<td>\n",
    "[[ 0.          0.        ] <br>\n",
    " [ 5.05844394 -1.68282702] <br>\n",
    " [ 0.          0.        ]]\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "mode = average\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "\n",
    "**mean of dA =**\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "0.145713902729\n",
    "\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "<td>\n",
    "**dA_prev[1,1] =** \n",
    "</td>\n",
    "<td>\n",
    "[[ 0.08485462  0.2787552 ] <br>\n",
    " [ 1.26461098 -0.25749373] <br>\n",
    " [ 1.17975636 -0.53624893]]\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations !\n",
    "\n",
    "Congratulation on completing this assignment. You now understand how convolutional neural networks work. You have implemented all the building blocks of a neural network. In the next assignment you will implement a ConvNet using TensorFlow."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "qO8ng",
   "launcher_item_id": "7XDi8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
